import functools as ft
import jax
import jax.core
import jax.extend.core
from jax.interpreters import mlir, batching
import jax.numpy as jnp
from jaxtyping import Array
import numpy as np
import equinox as eqx

# Force JAX to initialize CUDA context BEFORE importing my C++ functions!!!!!!!!
jax.devices()

# Import the functions that return pointers from our compiled C++
from spineax import single_solve, batch_solve, pbatch_solve

# primitives ===================================================================
# single
solve_single_f32_p = jax.extend.core.Primitive("solve_single_f32")
solve_single_f32_p.multiple_results = True
solve_single_f64_p = jax.extend.core.Primitive("solve_single_f64")
solve_single_f64_p.multiple_results = True
solve_single_c64_p = jax.extend.core.Primitive("solve_single_c64")
solve_single_c64_p.multiple_results = True
solve_single_c128_p = jax.extend.core.Primitive("solve_single_c128")
solve_single_c128_p.multiple_results = True

# batch
solve_batch_f32_p = jax.extend.core.Primitive("solve_batch_f32")
solve_batch_f32_p.multiple_results = True
solve_batch_f64_p = jax.extend.core.Primitive("solve_batch_f64")
solve_batch_f64_p.multiple_results = True
solve_batch_c64_p = jax.extend.core.Primitive("solve_batch_c64")
solve_batch_c64_p.multiple_results = True
solve_batch_c128_p = jax.extend.core.Primitive("solve_batch_c128")
solve_batch_c128_p.multiple_results = True

# pseudo batch
solve_pbatch_f32_p = jax.extend.core.Primitive("solve_pbatch_f32")
solve_pbatch_f32_p.multiple_results = True
solve_pbatch_f64_p = jax.extend.core.Primitive("solve_pbatch_f64")
solve_pbatch_f64_p.multiple_results = True
solve_pbatch_c64_p = jax.extend.core.Primitive("solve_pbatch_c64")
solve_pbatch_c64_p.multiple_results = True
solve_pbatch_c128_p = jax.extend.core.Primitive("solve_pbatch_c128")
solve_pbatch_c128_p.multiple_results = True

# Helper function to compute inertia from diag and perm
def compute_inertia_from_diag_perm(diag, perm, batch_size, matrix_dim):
    # Reorder diagonal according to permutation
    inv_perm = jnp.argsort(perm)
    diag_original_order = diag[inv_perm]
    out = diag_original_order.reshape([batch_size, matrix_dim])

    # cuDSS pivoting threshold seems to be 1e-13. everything above this on
    # plus or minus side seems to reliably indicate that particular inertia value.
    threshold = 1e-13
    positive = jnp.sum(out > threshold, axis=1)
    negative = jnp.sum(out < -threshold, axis=1)

    return jnp.stack([positive, negative], axis=1, dtype=jnp.int32)

# implementations ==============================================================
@solve_single_f32_p.def_impl
def solve_single_f32_impl(*args, **kwargs):
    return general_single_solve_impl("solve_single_f32", *args, **kwargs)
@solve_single_f64_p.def_impl
def solve_single_f64_impl(*args, **kwargs):
    return general_single_solve_impl("solve_single_f64", *args, **kwargs)
@solve_single_c64_p.def_impl
def solve_single_c64_impl(*args, **kwargs):
    return general_single_solve_impl("solve_single_c64", *args, **kwargs)
@solve_single_c128_p.def_impl
def solve_single_c128_impl(*args, **kwargs):
    return general_single_solve_impl("solve_single_c128", *args, **kwargs)

def general_single_solve_impl(
        name, 
        b_values, 
        csr_values,
        refit,
        csr_offsets,
        csr_columns,
        device_id, 
        mtype_id, 
        mview_id,
        ir_nsteps
    ):

    call = jax.ffi.ffi_call(
        name,
        (
            jax.ShapeDtypeStruct(b_values.shape, b_values.dtype),   # x
            jax.ShapeDtypeStruct(b_values.shape, b_values.dtype),   # diag
            jax.ShapeDtypeStruct(b_values.shape, jnp.int32),        # perm_reorder_row
        ),
        has_side_effect=True
    )

    x, diag, perm = call(
        b_values, 
        csr_values,
        refit, 
        csr_offsets,
        csr_columns,
        device_id = device_id, 
        mtype_id = mtype_id,
        mview_id = mview_id,
        ir_nsteps = ir_nsteps
    )

    # Compute inertia instead of returning diag and perm
    batch_size = 1
    matrix_dim = b_values.shape[0]
    inertia = compute_inertia_from_diag_perm(diag, perm, batch_size, matrix_dim)
    return [x, inertia[0]]  # Return solution and inertia for single batch

@solve_batch_f32_p.def_impl
def solve_batch_f32_impl(*args, **kwargs):
    return general_batch_solve_impl("solve_batch_f32", *args, **kwargs)
@solve_batch_f64_p.def_impl
def solve_batch_f64_impl(*args, **kwargs):
    return general_batch_solve_impl("solve_batch_f64", *args, **kwargs)
@solve_batch_c64_p.def_impl
def solve_batch_c64_impl(*args, **kwargs):
    return general_batch_solve_impl("solve_batch_c64", *args, **kwargs)
@solve_batch_c128_p.def_impl
def solve_batch_c128_impl(*args, **kwargs):
    return general_batch_solve_impl("solve_batch_c128", *args, **kwargs)

def general_batch_solve_impl(
        name, 
        b_values, 
        csr_values, 
        csr_offsets,
        csr_columns,
        batch_size,
        device_id, 
        mtype_id, 
        mview_id
    ):

    call = jax.ffi.ffi_call(
        name,
        (
            jax.ShapeDtypeStruct(b_values.shape, b_values.dtype),   # x
            jax.ShapeDtypeStruct((b_values.size,), b_values.dtype),   # diag
            jax.ShapeDtypeStruct((b_values.size,), jnp.int32),        # perm_reorder_row
        ),
        has_side_effect=True
    )

    x, diag, perm = call(
        b_values, 
        csr_values, 
        csr_offsets,
        csr_columns,
        batch_size = batch_size,
        device_id = device_id, 
        mtype_id = mtype_id,
        mview_id = mview_id,
    )

    # Compute inertia instead of returning diag and perm
    matrix_dim = b_values.shape[1]  # Assuming b_values shape is (batch_size, n)
    inertia = compute_inertia_from_diag_perm(diag, perm, batch_size, matrix_dim)
    return [x, inertia]


@solve_pbatch_f32_p.def_impl
def solve_pbatch_f32_impl(*args, **kwargs):
    return general_pbatch_solve_impl("solve_pbatch_f32", *args, **kwargs)
@solve_pbatch_f64_p.def_impl
def solve_pbatch_f64_impl(*args, **kwargs):
    return general_pbatch_solve_impl("solve_pbatch_f64", *args, **kwargs)
@solve_pbatch_c64_p.def_impl
def solve_pbatch_c64_impl(*args, **kwargs):
    return general_pbatch_solve_impl("solve_pbatch_c64", *args, **kwargs)
@solve_pbatch_c128_p.def_impl
def solve_pbatch_c128_impl(*args, **kwargs):
    return general_pbatch_solve_impl("solve_pbatch_c128", *args, **kwargs)

def general_pbatch_solve_impl(
        name, 
        b_values, 
        csr_values, 
        csr_offsets,
        csr_columns,
        batch_size,
        device_id, 
        mtype_id, 
        mview_id
    ):

    call = jax.ffi.ffi_call(
        name,
        (
            jax.ShapeDtypeStruct(b_values.shape, b_values.dtype),   # x
            jax.ShapeDtypeStruct((b_values.size,), b_values.dtype),   # diag
            jax.ShapeDtypeStruct((b_values.size,), jnp.int32),        # perm_reorder_row
        ),
        has_side_effect=True
    )

    x, diag, perm = call(
        b_values, 
        csr_values, 
        csr_offsets,
        csr_columns,
        batch_size = batch_size,
        device_id = device_id, 
        mtype_id = mtype_id,
        mview_id = mview_id,
    )

    # Compute inertia instead of returning diag and perm
    matrix_dim = b_values.shape[1]  # Assuming b_values shape is (batch_size, n)
    inertia = compute_inertia_from_diag_perm(diag, perm, batch_size, matrix_dim)
    jax.debug.print("inertia: {}", inertia)
    return [x, inertia]

# registrations and lowerings ==================================================

# single
jax.ffi.register_ffi_target("solve_single_f32", single_solve.handler_f32(), platform="CUDA")
jax.ffi.register_ffi_type_id("solve_single_f32", single_solve.type_id_f32(), platform="CUDA")
jax.ffi.register_ffi_target("solve_single_f64", single_solve.handler_f64(), platform="CUDA")
jax.ffi.register_ffi_type_id("solve_single_f64", single_solve.type_id_f64(), platform="CUDA")
jax.ffi.register_ffi_target("solve_single_c64", single_solve.handler_c64(), platform="CUDA")
jax.ffi.register_ffi_type_id("solve_single_c64", single_solve.type_id_c64(), platform="CUDA")
jax.ffi.register_ffi_target("solve_single_c128", single_solve.handler_c128(), platform="CUDA")
jax.ffi.register_ffi_type_id("solve_single_c128", single_solve.type_id_c128(), platform="CUDA")

solve_single_f32_low = mlir.lower_fun(solve_single_f32_impl, multiple_results=True)
mlir.register_lowering(solve_single_f32_p, solve_single_f32_low)
solve_single_f64_low = mlir.lower_fun(solve_single_f64_impl, multiple_results=True)
mlir.register_lowering(solve_single_f64_p, solve_single_f64_low)
solve_single_c64_low = mlir.lower_fun(solve_single_c64_impl, multiple_results=True)
mlir.register_lowering(solve_single_c64_p, solve_single_c64_low)
solve_single_c128_low = mlir.lower_fun(solve_single_c128_impl, multiple_results=True)
mlir.register_lowering(solve_single_c128_p, solve_single_c128_low)

# batch
jax.ffi.register_ffi_target("solve_batch_f32", batch_solve.handler_f32(), platform="CUDA")
jax.ffi.register_ffi_type_id("solve_batch_f32", batch_solve.type_id_f32(), platform="CUDA")
jax.ffi.register_ffi_target("solve_batch_f64", batch_solve.handler_f64(), platform="CUDA")
jax.ffi.register_ffi_type_id("solve_batch_f64", batch_solve.type_id_f64(), platform="CUDA")
jax.ffi.register_ffi_target("solve_batch_c64", batch_solve.handler_c64(), platform="CUDA")
jax.ffi.register_ffi_type_id("solve_batch_c64", batch_solve.type_id_c64(), platform="CUDA")
jax.ffi.register_ffi_target("solve_batch_c128", batch_solve.handler_c128(), platform="CUDA")
jax.ffi.register_ffi_type_id("solve_batch_c128", batch_solve.type_id_c128(), platform="CUDA")

solve_batch_f32_low = mlir.lower_fun(solve_batch_f32_impl, multiple_results=True)
mlir.register_lowering(solve_batch_f32_p, solve_batch_f32_low)
solve_batch_f64_low = mlir.lower_fun(solve_batch_f64_impl, multiple_results=True)
mlir.register_lowering(solve_batch_f64_p, solve_batch_f64_low)
solve_batch_c64_low = mlir.lower_fun(solve_batch_c64_impl, multiple_results=True)
mlir.register_lowering(solve_batch_c64_p, solve_batch_c64_low)
solve_batch_c128_low = mlir.lower_fun(solve_batch_c128_impl, multiple_results=True)
mlir.register_lowering(solve_batch_c128_p, solve_batch_c128_low)

# psuedo batch
jax.ffi.register_ffi_target("solve_pbatch_f32", pbatch_solve.handler_f32(), platform="CUDA")
jax.ffi.register_ffi_type_id("solve_pbatch_f32", pbatch_solve.type_id_f32(), platform="CUDA")
jax.ffi.register_ffi_target("solve_pbatch_f64", pbatch_solve.handler_f64(), platform="CUDA")
jax.ffi.register_ffi_type_id("solve_pbatch_f64", pbatch_solve.type_id_f64(), platform="CUDA")
jax.ffi.register_ffi_target("solve_pbatch_c64", pbatch_solve.handler_c64(), platform="CUDA")
jax.ffi.register_ffi_type_id("solve_pbatch_c64", pbatch_solve.type_id_c64(), platform="CUDA")
jax.ffi.register_ffi_target("solve_pbatch_c128", pbatch_solve.handler_c128(), platform="CUDA")
jax.ffi.register_ffi_type_id("solve_pbatch_c128", pbatch_solve.type_id_c128(), platform="CUDA")

solve_pbatch_f32_low = mlir.lower_fun(solve_pbatch_f32_impl, multiple_results=True)
mlir.register_lowering(solve_pbatch_f32_p, solve_pbatch_f32_low)
solve_pbatch_f64_low = mlir.lower_fun(solve_pbatch_f64_impl, multiple_results=True)
mlir.register_lowering(solve_pbatch_f64_p, solve_pbatch_f64_low)
solve_pbatch_c64_low = mlir.lower_fun(solve_pbatch_c64_impl, multiple_results=True)
mlir.register_lowering(solve_pbatch_c64_p, solve_pbatch_c64_low)
solve_pbatch_c128_low = mlir.lower_fun(solve_pbatch_c128_impl, multiple_results=True)
mlir.register_lowering(solve_pbatch_c128_p, solve_pbatch_c128_low)

# abstract evaluations =========================================================
@solve_single_f32_p.def_abstract_eval
@solve_single_f64_p.def_abstract_eval
@solve_single_c64_p.def_abstract_eval
@solve_single_c128_p.def_abstract_eval
def solve_aval(
        b_values, 
        csr_values, 
        refit, # new arg
        csr_offsets,
        csr_columns,
        device_id, 
        mtype_id, 
        mview_id,
        ir_nsteps # new attr
    ):
    return [
            jax.core.ShapedArray(b_values.shape, b_values.dtype),       # x
            jax.core.ShapedArray((2,), jnp.int32),                      # inertia [positive, negative]
        ]

@solve_batch_f32_p.def_abstract_eval
@solve_batch_f64_p.def_abstract_eval
@solve_batch_c64_p.def_abstract_eval
@solve_batch_c128_p.def_abstract_eval
def solve_batch_aval(
        b_values, 
        csr_values, 
        csr_offsets,
        csr_columns,
        batch_size,
        device_id, 
        mtype_id, 
        mview_id
    ):
    return [
            jax.core.ShapedArray(b_values.shape, b_values.dtype),       # x
            jax.core.ShapedArray((batch_size, 2), jnp.int32),           # inertia [positive, negative]
        ]

@solve_pbatch_f32_p.def_abstract_eval
@solve_pbatch_f64_p.def_abstract_eval
@solve_pbatch_c64_p.def_abstract_eval
@solve_pbatch_c128_p.def_abstract_eval
def solve_pbatch_aval(
        b_values, 
        csr_values, 
        csr_offsets,
        csr_columns,
        batch_size,
        device_id, 
        mtype_id, 
        mview_id
    ):
    return [
            jax.core.ShapedArray(b_values.shape, b_values.dtype),       # x
            jax.core.ShapedArray((batch_size, 2), jnp.int32),           # inertia [positive, negative]
        ]

# single solve interface =======================================================
@ft.partial(
    jax.jit, static_argnames=[
        "device_id",
        "mtype_id",
        "mview_id",
        "ir_nsteps" # New static attribute
    ]
)
def solve(
        b_values, 
        csr_values, 
        csr_offsets,
        csr_columns,
        device_id, 
        mtype_id, 
        mview_id,
        refit=True, # New boolean arg (default True)
        ir_nsteps=5 # New config (default 5)
    ):
    if csr_values.dtype == jnp.float32:
        # print(f"solving with float32")
        solver = solve_single_f32_p
    elif csr_values.dtype == jnp.float64:
        # print(f"solving with float64")
        solver = solve_single_f64_p
    elif csr_values.dtype == jnp.complex64:
        solver = solve_single_c64_p
    elif csr_values.dtype == jnp.complex128:
        solver = solve_single_c128_p
    else:
        raise ValueError(f"Unsupported dtype: {csr_values.dtype}")

    # Convert boolean refit to uint8 for FFI passing
    refit_u8 = jnp.array(refit, dtype=jnp.uint8)

    return solver.bind(
        b_values, 
        csr_values, 
        refit_u8,
        csr_offsets,
        csr_columns,
        device_id = device_id, 
        mtype_id = mtype_id,
        mview_id = mview_id,
        ir_nsteps = ir_nsteps
    )

# manual batch solve interface =================================================
@ft.partial(
    jax.jit, static_argnames=[
        "batch_size",
        "device_id",
        "mtype_id",
        "mview_id"
    ]
)
def batch_solve(
        b_values, 
        csr_values, 
        csr_offsets,
        csr_columns,
        batch_size,
        device_id, 
        mtype_id, 
        mview_id
    ):
    if csr_values.dtype == jnp.float32:
        print(f"solving with float32")
        solver = solve_batch_f32_p
    elif csr_values.dtype == jnp.float64:
        print(f"solving with float64")
        solver = solve_batch_f64_p
    elif csr_values.dtype == jnp.complex64:
        solver = solve_batch_c64_p
    elif csr_values.dtype == jnp.complex128:
        solver = solve_batch_c128_p
    else:
        raise ValueError(f"Unsupported dtype: {csr_values.dtype}")

    return solver.bind(
        b_values, 
        csr_values, 
        csr_offsets,
        csr_columns,
        batch_size = batch_size,
        device_id = device_id, 
        mtype_id = mtype_id,
        mview_id = mview_id,
    )


# manual psuedo batch solve interface =================================================
@ft.partial(
    jax.jit, static_argnames=[
        "batch_size",
        "device_id",
        "mtype_id",
        "mview_id"
    ]
)
def pbatch_solve(
        b_values, 
        csr_values, 
        csr_offsets,
        csr_columns,
        batch_size,
        device_id, 
        mtype_id, 
        mview_id
    ):
    if csr_values.dtype == jnp.float32:
        print(f"solving with float32")
        solver = solve_pbatch_f32_p
    elif csr_values.dtype == jnp.float64:
        print(f"solving with float64")
        solver = solve_pbatch_f64_p
    elif csr_values.dtype == jnp.complex64:
        solver = solve_pbatch_c64_p
    elif csr_values.dtype == jnp.complex128:
        solver = solve_pbatch_c128_p
    else:
        raise ValueError(f"Unsupported dtype: {csr_values.dtype}")

    return solver.bind(
        b_values, 
        csr_values, 
        csr_offsets,
        csr_columns,
        batch_size = batch_size,
        device_id = device_id, 
        mtype_id = mtype_id,
        mview_id = mview_id,
    )

# vmap batch solve interface ===================================================

def solve_single_f32_vmap(vector_arg_values, batch_axes, **kwargs):
    return general_solve_vmap(vector_arg_values, batch_axes, **kwargs)
def solve_single_f64_vmap(vector_arg_values, batch_axes, **kwargs):
    return general_solve_vmap(vector_arg_values, batch_axes, **kwargs)
def solve_single_c64_vmap(vector_arg_values, batch_axes, **kwargs):
    return general_solve_vmap(vector_arg_values, batch_axes, **kwargs)
def solve_single_c128_vmap(vector_arg_values, batch_axes, **kwargs):
    return general_solve_vmap(vector_arg_values, batch_axes, **kwargs)

vmap_using_pseudo_batch = True

def general_solve_vmap(
    vector_arg_values: tuple[Array, Array, Array, Array, Array],      # [b, csr_val, refit, off, col]
    batch_axes: tuple[int | None, int | None, int | None, int | None, int | None], 
    **kwargs                                                    # static params
) -> Array:

    b_values, csr_values, refit, csr_offsets, csr_columns = vector_arg_values
    a_b, a_val, a_refit, a_off, a_col = batch_axes

    # Handle spurious batch axes on sparsity patterns and refit
    if a_off is not None:
        csr_offsets = jax.lax.index_in_dim(csr_offsets, 0, axis=a_off, keepdims=False)
        a_off = None
    if a_col is not None:
        csr_columns = jax.lax.index_in_dim(csr_columns, 0, axis=a_col, keepdims=False)
        a_col = None
    if a_refit is not None:
         # For now, we don't support per-batch-element refit decisions in vmap. 
         # All batch elements must either refit or not.
         refit = jax.lax.index_in_dim(refit, 0, axis=a_refit, keepdims=False)
         a_refit = None

    # Update vector_arg_values with the corrected sparsity patterns
    vector_arg_values = (b_values, csr_values, csr_offsets, csr_columns) # Drop refit for batch call

    # guards (these should never trigger now since we handle spurious batch axes above)
    if any(ax is not None for ax in (a_off, a_col)):
        raise NotImplementedError("don't support batches of heterogeneous sparsity patterns yet (its coming tho...)")

    if all(ax is None for ax in (a_val, a_b)):
        raise NotImplementedError("Only batched csr_values and b_values are supported right now")
    
    # the non-batched path
    if a_val is None and a_b is None:
        # Re-insert refit
        return solve(b_values, csr_values, csr_offsets, csr_columns, refit=refit, **kwargs), (None, None)

    # if only one of the sets of values are batched
    elif a_val or a_b is None:
        raise NotImplementedError("Both csr_values and b_values must be batched")
    
    # the batched path binding
    elif a_val is not None and a_b is not None and vmap_using_pseudo_batch is False:
        if csr_values.dtype == jnp.float32:
            solver = solve_batch_f32_p
        elif csr_values.dtype == jnp.float64:
            solver = solve_batch_f64_p
        elif csr_values.dtype == jnp.complex64:
            solver = solve_batch_c64_p
        elif csr_values.dtype == jnp.complex128:
            solver = solve_batch_c128_p
        else:
            raise ValueError(f"Unsupported dtype: {csr_values.dtype}")
        
        # Note: Batch solve does not currently support refit=False optimization
        # It always refactors or solves based on its own internal counter logic (which we haven't updated yet)
        if kwargs.get('ir_nsteps', 5) != 5:
             print("Warning: ir_nsteps ignored for batched solve")
        kwargs.pop('ir_nsteps', None)

        return solver.bind(*vector_arg_values, batch_size=b_values.shape[0], **kwargs), (0,0)

    elif a_val is not None and a_b is not None and vmap_using_pseudo_batch is True:
        if csr_values.dtype == jnp.float32:
            solver = solve_pbatch_f32_p
        elif csr_values.dtype == jnp.float64:
            solver = solve_pbatch_f64_p
        elif csr_values.dtype == jnp.complex64:
            solver = solve_pbatch_c64_p
        elif csr_values.dtype == jnp.complex128:
            solver = solve_pbatch_c128_p
        else:
            raise ValueError(f"Unsupported dtype: {csr_values.dtype}")

        if kwargs.get('ir_nsteps', 5) != 5:
             print("Warning: ir_nsteps ignored for batched solve")
        kwargs.pop('ir_nsteps', None)

        return solver.bind(*vector_arg_values, batch_size=b_values.shape[0], **kwargs), (0,0)
    
    else:
        raise NotImplementedError("This path should not be possible")

batching.primitive_batchers[solve_single_f32_p] = solve_single_f32_vmap
batching.primitive_batchers[solve_single_f64_p] = solve_single_f64_vmap
batching.primitive_batchers[solve_single_c64_p] = solve_single_c64_vmap
batching.primitive_batchers[solve_single_c128_p] = solve_single_c128_vmap

# vmap of vmap
def solve_batch_vmap(vector_arg_values, batch_axes, **kwargs):
    """Handle vmap of already-batched solve"""
    b_values, csr_values, csr_offsets, csr_columns = vector_arg_values
    a_b, a_val, a_off, a_col = batch_axes

    # Handle spurious batch axes on sparsity patterns (same fix as general_solve_vmap)
    if a_off is not None:
        csr_offsets = jax.lax.index_in_dim(csr_offsets, 0, axis=a_off, keepdims=False)
        a_off = None
    if a_col is not None:
        csr_columns = jax.lax.index_in_dim(csr_columns, 0, axis=a_col, keepdims=False)
        a_col = None
    vector_arg_values = (b_values, csr_values, csr_offsets, csr_columns)

    if any(ax is not None for ax in (a_off, a_col)):
        raise NotImplementedError("don't support batches of heterogeneous sparsity patterns yet (its coming tho...)")

    if a_b is None and a_val is None:
        # Not actually batching
        return batch_solve(*vector_arg_values, **kwargs), (None, None)
    
    # Flatten nested batches
    batch_size1 = b_values.shape[0]
    batch_size2 = b_values.shape[1]
    total_batch = batch_size1 * batch_size2
    
    b_flat = b_values.reshape(total_batch, -1)
    csr_flat = csr_values.reshape(total_batch, -1)
    
    # the non-batched path
    if a_val is None and a_b is None:
        return solve(*vector_arg_values, **kwargs), (None, None)

    # if only one of the sets of values are batched
    elif a_val or a_b is None:
        raise NotImplementedError("Both csr_values and b_values must be batched")

    elif a_val is not None and a_b is not None and vmap_using_pseudo_batch is False:
        if csr_values.dtype == jnp.float32:
            solver = solve_batch_f32_p
        elif csr_values.dtype == jnp.float64:
            solver = solve_batch_f64_p
        elif csr_values.dtype == jnp.complex64:
            solver = solve_batch_c64_p
        elif csr_values.dtype == jnp.complex128:
            solver = solve_batch_c128_p
        else:
            raise ValueError(f"Unsupported dtype: {csr_values.dtype}")
        return solver.bind(*vector_arg_values, batch_size=b_values.shape[0], **kwargs), (0,0)
    elif a_val is not None and a_b is not None and vmap_using_pseudo_batch is True:
        if csr_values.dtype == jnp.float32:
            solver = solve_pbatch_f32_p
        elif csr_values.dtype == jnp.float64:
            solver = solve_pbatch_f64_p
        elif csr_values.dtype == jnp.complex64:
            solver = solve_pbatch_c64_p
        elif csr_values.dtype == jnp.complex128:
            solver = solve_pbatch_c128_p
        else:
            raise ValueError(f"Unsupported dtype: {csr_values.dtype}")

    # we are replacing this
    kwargs.__delitem__("batch_size")

    x_flat, inertia_flat = solver.bind(
        b_flat, csr_flat, csr_offsets, csr_columns,
        batch_size=total_batch, 
        **kwargs
    )
    
    # Reshape back
    x = x_flat.reshape(batch_size1, batch_size2, -1)
    inertia = inertia_flat.reshape(batch_size1, batch_size2, 2)
    
    return (x, inertia), (0, 0)

batching.primitive_batchers[solve_batch_f32_p] = solve_batch_vmap
batching.primitive_batchers[solve_batch_f64_p] = solve_batch_vmap
batching.primitive_batchers[solve_pbatch_f32_p] = solve_batch_vmap
batching.primitive_batchers[solve_pbatch_f64_p] = solve_batch_vmap

# create python side composable class to ensure validity of the columns and offsets
class CuDSSSolver(eqx.Module):
    """Sparse linear solver wrapper that marks sparsity pattern as static for vmap."""
    csr_offsets: Array = eqx.field(static=True)
    csr_columns: Array = eqx.field(static=True)
    device_id: int = eqx.field(static=True)
    mtype_id: int = eqx.field(static=True)
    mview_id: int = eqx.field(static=True)
    ir_nsteps: int = eqx.field(static=True) # New field

    def __init__(self, csr_offsets, csr_columns, device_id, mtype_id, mview_id, ir_nsteps=5):
        self.csr_offsets = csr_offsets
        self.csr_columns = csr_columns
        self.device_id = device_id
        self.mtype_id = mtype_id
        self.mview_id = mview_id
        self.ir_nsteps = ir_nsteps

    def __call__(self, b, csr_values, refit=True):
        return solve(b, csr_values,
            csr_offsets=self.csr_offsets,
            csr_columns=self.csr_columns,
            device_id=self.device_id,
            mtype_id=self.mtype_id,
            mview_id=self.mview_id,
            ir_nsteps=self.ir_nsteps,
            refit=refit
        )   

if __name__ == "__main__":

    import jax.experimental.sparse as jsparse

    # example usage
    # -------------
    M1 = jnp.array([
        [4., 0., 1., 0., 0.],
        [0., 3., 2., 0., 0.],
        [0., 0., 5., 0., 1.],
        [0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 2.],
    ])
    M2 = M1 * 0.9

    b1 = jnp.array([7.0, 12.0, 25.0, 4.0, 13.0])
    b2 = b1 * 1.1

    m1 = M1 + M1.T - jnp.diag(M1) * jnp.eye(M1.shape[0])
    m2 = M2 + M2.T - jnp.diag(M2) * jnp.eye(M2.shape[0])
    true_x1 = jnp.linalg.solve(m1, b1)
    true_x2 = jnp.linalg.solve(m2, b2)

    LHS1 = jsparse.BCSR.fromdense(M1)
    LHS2 = jsparse.BCSR.fromdense(M2)
    csr_offsets1, csr_columns1, csr_values1 = LHS1.indptr, LHS1.indices, LHS1.data
    csr_offsets2, csr_columns2, csr_values2 = LHS2.indptr, LHS2.indices, LHS2.data

    assert all(csr_offsets1 == csr_offsets2)
    assert all(csr_columns1 == csr_columns2)

    batch_size = 2
    offsets_batch = jnp.vstack([csr_offsets1, csr_offsets2])
    columns_batch = jnp.vstack([csr_columns1, csr_columns2])
    csr_values = jnp.vstack([csr_values1, csr_values2])
    device_id = 0; mtype_id = 1; mview_id = 1
    b = jnp.vstack([b1, b2])

    # instantiate solve
    solver = CuDSSSolver(csr_offsets1, csr_columns1, device_id, mtype_id, mview_id)

    # call it - dispatches single solve by default
    test1, in1 = solver(b[0], csr_values[0])

    # call it in vmap/jit
    test2, in2 = jax.jit(jax.vmap(solver))(b, csr_values)

    # unlimited composability in jit/vmap
    b_ = jnp.stack([jnp.stack([b,b]), jnp.stack([b,b])])
    csr_values_ = jnp.stack([jnp.stack([csr_values, csr_values]), jnp.stack([csr_values, csr_values])])
    test3, in3 = jax.jit(jax.vmap(jax.vmap(jax.vmap(solver))))(b_, csr_values_)

    # see difference between dense solves and cuDSS
    pass


